{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seandaytoy1005/MMO/blob/main/lab9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируйте библиотеки, необходимые для предварительной обработки набора данных, разработки функций и обучения модели."
      ],
      "metadata": {
        "id": "YVBHM95PooWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "metadata": {
        "id": "JdBZQi2colQS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple\n",
        "from scipy import stats\n",
        "from IPython.display import Image\n",
        "# from sklearn.datasets import load_iris, load_boston\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set(style=\"ticks\")"
      ],
      "metadata": {
        "id": "qBi0g7KE2262"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этой статье я использую набор обзорных данных Amazon."
      ],
      "metadata": {
        "id": "fBcMuriBo4B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#加载数据集\n",
        "data = open('/content/corpus').read()\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "  content = line.split()\n",
        "  labels.append(content[0])\n",
        "  texts.append(content[1])\n",
        "\n",
        "#创建一个dataframe，列名为text和label\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels"
      ],
      "metadata": {
        "id": "1cYYbD2bo3gy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p2RM7AapEEM",
        "outputId": "2867a3ed-4c9d-43cf-aea3-ed6c4a5ed1e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             text       label\n",
            "0         Stuning  __label__2\n",
            "1             The  __label__2\n",
            "2       Amazing!:  __label__2\n",
            "3       Excellent  __label__2\n",
            "4       Remember,  __label__2\n",
            "...           ...         ...\n",
            "9995            A  __label__2\n",
            "9996        Great  __label__2\n",
            "9997  Interesting  __label__1\n",
            "9998        Don't  __label__1\n",
            "9999    Beautiful  __label__2\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затем мы разделяем набор данных на обучающий и проверочный наборы, чтобы можно было обучать и тестировать классификатор. Кроме того, мы закодируем наш целевой столбец, чтобы его можно было использовать в моделях машинного обучения:"
      ],
      "metadata": {
        "id": "HwiMQeZ9pQy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#将数据集分为训练集和验证集\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# label编码为目标变量\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "metadata": {
        "id": "QrZ0T1Inoz3c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее следует разработка объектов. На этом этапе исходные данные будут преобразованы в векторы объектов, а новые объекты будут созданы на основе существующих данных. Чтобы выбрать важные признаки из набора данных, существует несколько способов:\n",
        "* Считать векторы объектами.\n",
        "* Вектор TF-IDF как признак\n",
        "* Встраивание слов как функции\n",
        "* Функции на основе текста/НЛП\n",
        "* Тематические модели как функции"
      ],
      "metadata": {
        "id": "t7FfGkrup16j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer"
      ],
      "metadata": {
        "id": "4Xu9YjzwqaAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "#使用向量计数器对象转换训练集和验证集\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "metadata": {
        "id": "wcoOSHHAqYj-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Векторы TF-IDF как признаки"
      ],
      "metadata": {
        "id": "24PHRLg6qsUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Использование [N-грамм](https://ru.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0)\n",
        "\n",
        "В компьютерной лингвистике существует гипотеза о том, что основными носителями смысла в предложении являются не слова, а словосочетания. Поэтому существует возможность объединения соседних слов в N-граммы."
      ],
      "metadata": {
        "id": "iVU0on0P7UoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#词语级tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram 级tf-idf\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "#词性级tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x)\n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhir-u9Uqg9I",
        "outputId": "12c4516f-aecb-4cb3-ec68-6f6f776ce5c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "встраивание слов\n",
        "\n",
        "Встраивание слов — это форма использования плотных векторов для представления слов и документов. Положение слова в векторном пространстве определяется из контекста слова в тексте. Встраивание слов можно обучить с использованием самого входного корпуса или сгенерировать с использованием предварительно обученной модели встраивания слов. Модели встраивания слов включают в себя: Перчатка, FastText, Word2Vec"
      ],
      "metadata": {
        "id": "UNo97Lt1q49v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec"
      ],
      "metadata": {
        "id": "stdtf1QMwKkQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkJe3hOS2G3O",
        "outputId": "0c6c933d-3cf0-4eeb-820a-1762e5051aa5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/wiki-news-300d-1M.vec.zip'"
      ],
      "metadata": {
        "id": "4RVvz-CGwSJ5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('/content/drive/MyDrive/研一/mmo/wiki-news-300d-1M.vec')):\n",
        "  values = line.split()\n",
        "  embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "RhGAux0oq_XL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Количество загруженных векторов встраивания слов:\", len(embeddings_index))\n",
        "print(\"Количество индексов слов для токенизатора:\", len(word_index))\n",
        "print(\"Создана форма карты встраивания сегментации слов.:\", embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "195g5YdV86ZU",
        "outputId": "ba3ea16d-41e6-4543-b3f8-a3d55a6eab59"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество загруженных векторов встраивания слов: 3620\n",
            "Количество индексов слов для токенизатора: 2586\n",
            "Создана форма карты встраивания сегментации слов.: (2587, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "\n",
        "data = open('/content/corpus').read()\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    labels.append(content[0])\n",
        "    texts.append(content[1])\n",
        "\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "\n",
        "model = Word2Vec([text.split() for text in texts], vector_size=300, window=5, min_count=1)\n",
        "\n",
        "embeddings_index = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "\n",
        "\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Bb9sAT2790bc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Параметры модели:\", model)\n",
        "print(\"Количество слов в токенайзере:\", len(token.word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wPOwNFv-Zgc",
        "outputId": "d48090f9-29d7-460c-c863-730224706e8d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Параметры модели: Word2Vec<vocab=3620, vector_size=300, alpha=0.025>\n",
            "Количество слов в токенайзере: 2586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используйте наивный байесовский классификатор (MultinomialNB) для обучения и прогнозирования функций, а также расчета точности модели."
      ],
      "metadata": {
        "id": "SxAyJv94--8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(xtrain_count, train_y)\n",
        "\n",
        "predictions = clf.predict(xvalid_count)\n",
        "accuracy = accuracy_score(valid_y, predictions)\n",
        "\n",
        "print(\"Accuracy ( CountVectorizer） of the model: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLV9hLflAn29",
        "outputId": "b4d5097a-2186-42bc-e402-126f3bf9e0fa"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy ( CountVectorizer） of the model: 69.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clf_word = MultinomialNB()\n",
        "clf_word.fit(xtrain_tfidf, train_y)\n",
        "predictions_word = clf_word.predict(xvalid_tfidf)\n",
        "accuracy_word = accuracy_score(valid_y, predictions_word)\n",
        "\n",
        "clf_ngram = MultinomialNB()\n",
        "clf_ngram.fit(xtrain_tfidf_ngram, train_y)\n",
        "predictions_ngram = clf_ngram.predict(xvalid_tfidf_ngram)\n",
        "accuracy_ngram = accuracy_score(valid_y, predictions_ngram)\n",
        "\n",
        "clf_chars = MultinomialNB()\n",
        "clf_chars.fit(xtrain_tfidf_ngram_chars, train_y)\n",
        "predictions_chars = clf_chars.predict(xvalid_tfidf_ngram_chars)\n",
        "accuracy_chars = accuracy_score(valid_y, predictions_chars)\n",
        "\n",
        "print(\"Accuracy (word-level tf-idf): {:.2f}%\".format(accuracy_word * 100))\n",
        "print(\"Accuracy (ngram-level tf-idf): {:.2f}%\".format(accuracy_ngram * 100))\n",
        "print(\"Accuracy (character-level tf-idf): {:.2f}%\".format(accuracy_chars * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0k9iHyuBhas",
        "outputId": "958d5365-ba47-4700-b979-6b8423dee964"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (word-level tf-idf): 69.76%\n",
            "Accuracy (ngram-level tf-idf): 50.44%\n",
            "Accuracy (character-level tf-idf): 67.08%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "352.675px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}